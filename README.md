# RAG from Scratch

**Demystify Retrieval-Augmented Generation (RAG) by building it yourself â€” step by step.**  
No black boxes. No cloud APIs. Just clear explanations, simple examples, and local code you fully understand.

This project follows the same philosophy as [AI Agents from Scratch](https://github.com/pguso/ai-agents-from-scratch):  
make advanced AI concepts approachable for developers through minimal, well-explained, real code.

---

## ğŸš€ What You'll Learn

- **What RAG really is** â€” and why itâ€™s so powerful for knowledge retrieval.  
- **How embeddings work** â€” turn text into numbers your model can understand.  
- **How to build a local vector database** â€” store and query documents efficiently.  
- **How to connect everything** â€” retrieve context and feed it into an LLM for grounded answers.  
- **Step-by-step code walkthroughs** â€” every function explained, nothing hidden.

---

## ğŸ§  Concept Overview

Retrieval-Augmented Generation (RAG) enhances language models by giving them access to **external knowledge**.  
Instead of asking the model to â€œrememberâ€ everything, you let it **retrieve relevant context** before generating a response.

**Pipeline:**
1. **Knowledge Requirements** question and answer sets to test availabilty of data sources and capabilities of the whole pipeline
2. **Embed** your documents (turn text into vectors).  
3. **Store** them in a vector database.  
4. **Retrieve** relevant context for a query.  
5. **Augment** your prompt with that context.  
6. **Generate** the final answer using a local LLM.

---

## ğŸ“‚ Project Structure

```
rag-from-scratch/
â”œâ”€â”€ 00_how_rag_works/
â”‚   â””â”€â”€ example.js
â”‚       // A minimal simulation of RAG using plain JS objects:
â”‚       // - small document array
â”‚       // - naive keyword search
â”‚       // - simple answer concatenation
â”‚       // Purpose: show the *idea* of retrieval + generation in one file.
â”œâ”€â”€ 01_knowledge_requirements/
â”‚ â””â”€â”€ example.js
â”‚       // Explain how enterprise or local data becomes the knowledge base.
â”‚       // Add a small dataset (like 5 FAQ entries) for hands-on experiments.
â”œâ”€â”€ 02_intro_to_embeddings/
â”‚   â”œâ”€â”€ 01_text_similarity_basics.js
â”‚   â”‚   // Compute similarity using cosine similarity on short numeric vectors.
â”‚   â”‚   // No real model â€” helps visualize the math intuition.
â”‚   â””â”€â”€ 02_generate_embeddings.js
â”‚       // Show how to convert text â†’ embeddings (e.g., via API or mock vectors).
â”œâ”€â”€ 03_building_vector_store/
â”‚   â”œâ”€â”€ 01_in_memory_store.js
â”‚   â”‚   // Build a mini vector store with JS arrays and cosine search.
â”‚   â”œâ”€â”€ 02_nearest_neighbor_search.js
â”‚   â”‚   // Visualize how nearest-neighbor search works in code.
â”‚   â””â”€â”€ example.js
â”‚       // Wrap both to simulate a minimal vector DB.
â”œâ”€â”€ 04_retrieval_pipeline/
â”‚   â”œâ”€â”€ 01_query_rewriting.js
â”‚   â”‚   // Show how rewriting affects retrieval results (e.g., synonyms).
â”‚   â”œâ”€â”€ 02_rank_results.js
â”‚   â”‚   // Implement simple scoring and ordering.
â”‚   â”œâ”€â”€ 03_no_results_check.js
â”‚   â”‚   // Add fallback logic for empty or poor matches.
â”‚   â””â”€â”€ example.js
â”‚       // Chain all parts to simulate the full retrieval flow.
â”œâ”€â”€ 05_rag_in_action/
â”‚   â””â”€â”€ example.js
â”‚       // Combine retrieval with a simple LLM stub (e.g., template-based generator).
â”‚       // Example: â€œBased on {context}, the answer is...â€
â”‚
â”œâ”€â”€ 06_evaluating_rag_quality/
â”‚ â””â”€â”€ example.js
â”‚     // Measure retrieval precision, recall, and LLM accuracy.
â”œâ”€â”€ 07_observability_and_caching/
â”‚ â””â”€â”€ example.js
â”‚     // Show caching repeated queries and logging performance.
â””â”€â”€ README.md
```

### How it works
| Goal                                | What You Add                                          | Why It Helps                                                                |
| ----------------------------------- | ----------------------------------------------------- | --------------------------------------------------------------------------- |
| **Concept clarity**                 | `00_how_rag_works`                                    | Lets users see retrieval + generation in <20 lines before touching vectors. |
| **Mathematical intuition**          | `02_intro_to_embeddings/01_text_similarity_basics.js` | Teaches cosine similarity without black-box APIs.                           |
| **Hands-on understanding**          | `03_building_vector_store/01_in_memory_store.js`      | Users see how vector databases actually store & compare embeddings.         |
| **Pipeline thinking**               | `04_retrieval_pipeline`                         | Each stage becomes a visible, testable function.                            |

Each folder contains:
- A **minimal example** (`example.js`)
- A **detailed explanation** of every step  
- Comments in the code to teach the concept clearly

---

## ğŸ§° Requirements

- Node.js 18+  
- Local LLM (node-llama-cpp)  
- npm packages for embeddings and vector math  

Install dependencies:

```bash
npm install
```

Run examples:

```
node 01_intro_to_embeddings/example.js
```

## Philosophy

This repository is not about fancy frameworks or huge models.
Itâ€™s about understanding â€” line by line â€” how RAG works under the hood.

If you can explain it, you can build it.
If you can build it, you can improve it.

## Contribute

Contributions are welcome!
If you have a clear, educational RAG example, open a PR.

See Also

[AI Agents from Scratch](https://github.com/pguso/ai-agents-from-scratch)
[LangChain RAG Concepts](https://docs.langchain.com/oss/python/langchain/rag)
[Best AI tools for RAG](https://codingscape.com/blog/best-ai-tools-for-retrieval-augmented-generation-rag)

